{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1daee2a-35fc-4c8d-9dba-83146111b32e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49825/121151160.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(neural_data, dtype=torch.float32), torch.tensor(continuous_index, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import cebra.datasets\n",
    "from cebra import CEBRA\n",
    "\n",
    "hippocampus_pos=cebra.datasets.init('rat-hippocampus-single-achilles')\n",
    "\n",
    "\n",
    "class Skip(nn.Module):\n",
    "    def __init__(self, *modules, crop=(1, 1)):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(*modules)\n",
    "        self.crop = slice(crop[0], -crop[1] if isinstance(crop[1], int) and crop[1] > 0 else None)\n",
    "\n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        skip = self.module(inp)\n",
    "        return inp[..., self.crop] + skip\n",
    "\n",
    "class Offset10Model(nn.Module):\n",
    "    def __init__(self, num_neurons, num_units, num_output, normalize=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv1d(num_neurons, num_units, 2),\n",
    "            nn.GELU(),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            nn.Conv1d(num_units, num_output, 3)\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self.norm = nn.LayerNorm(num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        if self.normalize:\n",
    "            x = x.transpose(1, 2)  # Move the output_dim to the last dimension for LayerNorm\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2)  # Move it back\n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "num_neurons = 120  # input channels for Conv1d\n",
    "num_units = 128\n",
    "output_dim = 3\n",
    "batch_size = 512\n",
    "learning_rate = 3e-4\n",
    "max_iterations = 10000\n",
    "\n",
    "# Prepare Dataset\n",
    "neural_data = hippocampus_pos.neural[:, None, :]  # Reshape to [10178, 1, 120]\n",
    "continuous_index = hippocampus_pos.continuous_index.numpy()\n",
    "dataset = TensorDataset(torch.tensor(neural_data, dtype=torch.float32), torch.tensor(continuous_index, dtype=torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Offset10Model(num_neurons=1, num_units=num_units, num_output=output_dim, normalize=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc29a264-4206-47fb-9ce5-bf6a758e8fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10178, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Skip(nn.Module):\n",
    "    def __init__(self, *modules, crop=(1, 1)):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(*modules)\n",
    "        self.crop = slice(crop[0], -crop[1] if isinstance(crop[1], int) and crop[1] > 0 else None)\n",
    "\n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        skip = self.module(inp)\n",
    "        return inp[..., self.crop] + skip\n",
    "\n",
    "class Offset10Model(nn.Module):\n",
    "    def __init__(self, num_neurons, num_units, num_output, normalize=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv1d(num_neurons, num_units, 2),\n",
    "            nn.GELU(),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            nn.Conv1d(num_units, num_output, 3)\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self.norm = nn.LayerNorm(num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        if self.normalize:\n",
    "            x = x.transpose(1, 2)  # Move the output_dim to the last dimension for LayerNorm\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2)  # Move it back\n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "num_neurons = 120  # input channels for Conv1d\n",
    "num_units = 128\n",
    "output_dim = 3\n",
    "batch_size = 512\n",
    "learning_rate = 3e-4\n",
    "max_iterations = 10000\n",
    "\n",
    "# Prepare Dataset\n",
    "neural_data = hippocampus_pos.neural[:, None, :]  # Reshape to [10178, 1, 120]\n",
    "continuous_index = hippocampus_pos.continuous_index.numpy()\n",
    "dataset = TensorDataset(torch.tensor(neural_data, dtype=torch.float32), torch.tensor(continuous_index, dtype=torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Offset10Model(num_neurons=1, num_units=num_units, num_output=output_dim, normalize=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5f6fe88-112d-4c92-88dd-17e737402aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(features, labels, temperature=0.07):\n",
    "    batch_size = features.shape[0]\n",
    "    features = features.view(batch_size, -1)  # Flatten features to [batch_size, output_dim]\n",
    "\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "\n",
    "    anchor_dot_contrast = torch.div(torch.matmul(features, features.T), temperature)\n",
    "\n",
    "    logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "    logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "    mask = mask.repeat(1, 1)\n",
    "    logits_mask = torch.scatter(\n",
    "        torch.ones_like(mask),\n",
    "        1,\n",
    "        torch.arange(batch_size).view(-1, 1).to(device),\n",
    "        0\n",
    "    )\n",
    "    mask = mask * logits_mask\n",
    "\n",
    "    exp_logits = torch.exp(logits) * logits_mask\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "    loss = -mean_log_prob_pos\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384c611-3cdc-424a-8d11-a32c7448d43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcafe6be-f76e-4102-b392-4ef82d40f8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (1536) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Remove the last dimension\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcontrastive_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[47], line 22\u001b[0m, in \u001b[0;36mcontrastive_loss\u001b[0;34m(features, labels, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m logits_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m     15\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones_like(mask),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(batch_size)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m logits_mask\n\u001b[0;32m---> 22\u001b[0m exp_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_mask\u001b[49m\n\u001b[1;32m     23\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(exp_logits\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     25\u001b[0m mean_log_prob_pos \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m*\u001b[39m log_prob)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (1536) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(max_iterations):\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze(-1)  # Remove the last dimension\n",
    "        loss = contrastive_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"cebra_behavior_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598ef10-6908-4c74-9ddd-defc5a74a0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
