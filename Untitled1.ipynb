{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53230293-2553-40eb-bd2f-15bf9ea4e85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53623/4026922199.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(neural_data, dtype=torch.float32), torch.tensor(continuous_index, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "import cebra\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Initialize the dataset\n",
    "hippocampus_pos = cebra.datasets.init('rat-hippocampus-single-achilles')\n",
    "\n",
    "# Prepare Dataset\n",
    "neural_data = hippocampus_pos.neural[:, None, :]  # Reshape to [10178, 1, 120]\n",
    "continuous_index = hippocampus_pos.continuous_index.numpy()\n",
    "dataset = TensorDataset(torch.tensor(neural_data, dtype=torch.float32), torch.tensor(continuous_index, dtype=torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1efc8fd-cc1c-4f9f-93f9-9829da2982fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Skip(nn.Module):\n",
    "    def __init__(self, *modules, crop=(1, 1)):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(*modules)\n",
    "        self.crop = slice(crop[0], -crop[1] if isinstance(crop[1], int) and crop[1] > 0 else None)\n",
    "\n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        skip = self.module(inp)\n",
    "        return inp[..., self.crop] + skip\n",
    "\n",
    "class Offset10Model(nn.Module):\n",
    "    def __init__(self, num_neurons, num_units, num_output, normalize=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv1d(num_neurons, num_units, 2),\n",
    "            nn.GELU(),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            Skip(nn.Conv1d(num_units, num_units, 3), nn.GELU()),\n",
    "            nn.Conv1d(num_units, num_output, 3)\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self.norm = nn.LayerNorm(num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        if self.normalize:\n",
    "            x = x.transpose(1, 2)  # Move the output_dim to the last dimension for LayerNorm\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2)  # Move it back\n",
    "        return x\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Offset10Model(num_neurons=1, num_units=128, num_output=3, normalize=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eea9e6-7c79-4c45-b212-2f775e28ab3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(features, labels, temperature=0.07):\n",
    "    batch_size = features.shape[0]\n",
    "    features = features.view(batch_size, -1)  # Flatten features to [batch_size, output_dim]\n",
    "\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "\n",
    "    anchor_dot_contrast = torch.div(torch.matmul(features, features.T), temperature)\n",
    "\n",
    "    logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "    logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "    mask = mask.repeat(1, batch_size)\n",
    "    logits_mask = torch.scatter(\n",
    "        torch.ones_like(mask),\n",
    "        1,\n",
    "        torch.arange(batch_size).view(-1, 1).to(device),\n",
    "        0\n",
    "    )\n",
    "    mask = mask * logits_mask\n",
    "\n",
    "    exp_logits = torch.exp(logits) * logits_mask\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "    loss = -mean_log_prob_pos\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2af99fd-602f-4168-8eb3-c7aba49c4023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (786432) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Remove the last dimension\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcontrastive_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mcontrastive_loss\u001b[0;34m(features, labels, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m logits_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m     15\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones_like(mask),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(batch_size)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m logits_mask\n\u001b[0;32m---> 22\u001b[0m exp_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_mask\u001b[49m\n\u001b[1;32m     23\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(exp_logits\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     25\u001b[0m mean_log_prob_pos \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m*\u001b[39m log_prob)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (786432) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10000):\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze(-1)  # Remove the last dimension\n",
    "        loss = contrastive_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"cebra_behavior_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7232e-df15-4f51-90da-44b399b65b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
