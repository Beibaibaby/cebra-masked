{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3025e15-968a-4f14-a12c-f095e2c810c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib as jl\n",
    "import cebra.datasets\n",
    "from cebra import CEBRA\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ae539e-4b00-4c3c-a2a4-4e21e351874c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70804/4069690478.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  neural_data = torch.tensor(cebra.datasets.init('rat-hippocampus-single-achilles').neural, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "hippocampus_pos = cebra.datasets.init('rat-hippocampus-single-achilles')\n",
    "neural_data = torch.tensor(cebra.datasets.init('rat-hippocampus-single-achilles').neural, dtype=torch.float32)\n",
    "labels = torch.tensor(cebra.datasets.init('rat-hippocampus-single-achilles').continuous_index.numpy(), dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b2d0a-d0e8-4460-8bd6-b2ac8c1c1e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee908420-d488-472f-b253-b55326e635f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNetVAE(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_units, latent_dim):\n",
    "        super(ConvNetVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_features, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_hidden_units, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_hidden_units, out_channels=latent_dim, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=latent_dim, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=num_hidden_units, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=num_hidden_units, out_channels=num_features, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=latent_dim, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=num_hidden_units, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=num_hidden_units, out_channels=num_features, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).permute(0, 2, 1)\n",
    "        #print(x.shape)\n",
    "        encoded = self.encoder(x)\n",
    "        #print(encoded.shape)\n",
    "        decoded = self.decoder(encoded)\n",
    "        #print(decoded.shape)\n",
    "        return encoded.squeeze(2), decoded.permute(0, 2, 1).squeeze(1)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Adjust latent dimension as needed\n",
    "latent_dim = 10  # This is a hyperparameter you can tune\n",
    "model = ConvNetVAE(num_features=120, num_hidden_units=16, latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567deb12-1d5b-4374-82d1-b098456ef4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruction_loss(reconstructed, original):\n",
    "    return F.mse_loss(reconstructed, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48ac257-5a82-4f76-b80a-82689281c665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataLoader setup\n",
    "dataset = TensorDataset(neural_data, labels)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746645a1-0e69-484c-a455-2981a57b19c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 100, Loss: 0.0499\n"
     ]
    }
   ],
   "source": [
    "def pretrain_epoch(loader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, _ in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, reconstructed = model(data)\n",
    "        loss = reconstruction_loss(reconstructed, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Pretrain the model\n",
    "pretrain_epochs = 100\n",
    "for epoch in range(pretrain_epochs):\n",
    "    loss = pretrain_epoch(loader, model, optimizer)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Pretrain Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb95bc2e-1ec9-4576-bd3a-f5fe09ec08c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info_nce_loss(features, labels, temperature=1.0, threshold=0.3):\n",
    "    # Normalize features to unit length for cosine similarity\n",
    "    features = F.normalize(features, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity matrix\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    \n",
    "    # Calculate Euclidean distances between labels for defining positive pairs\n",
    "    labels_diff = torch.cdist(labels[:,0].unsqueeze(1), labels[:,0].unsqueeze(1))\n",
    "    \n",
    "    # Create a mask for positive samples based on a threshold in label space\n",
    "    positive_mask = (labels_diff <= threshold).float()\n",
    "\n",
    "    # Calculate exponentiated similarities scaled by temperature\n",
    "    exp_sim = torch.exp(similarity_matrix / temperature)\n",
    "    \n",
    "    # Compute sums of exponentiated similarities where masks apply\n",
    "    pos_sum = torch.sum(exp_sim * positive_mask, dim=1)\n",
    "    all_sum = torch.sum(exp_sim, dim=1)\n",
    "    \n",
    "    # Calculate the InfoNCE loss\n",
    "    loss = -torch.log(pos_sum / all_sum + 1e-6)\n",
    "\n",
    "    return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ff221e-750c-4e75-a00f-efa915ff97d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally save the model state\n",
    "torch.save(model.state_dict(), 'pretrained_model.pth')\n",
    "\n",
    "\n",
    "# Load the entire state dict from the file\n",
    "full_state_dict = torch.load('pretrained_model.pth', map_location=device)\n",
    "\n",
    "# Extract just the encoder part of the state dict\n",
    "encoder_state_dict = {key.replace('encoder.', ''): value for key, value in full_state_dict.items() if key.startswith('encoder.')}\n",
    "\n",
    "# Load the adjusted state dict into the encoder\n",
    "model.encoder.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4df74f-8b7b-4ad5-91bf-9822708f8222",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConvNet' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Initialize model, optimizer\u001b[39;00m\n\u001b[1;32m     80\u001b[0m model \u001b[38;5;241m=\u001b[39m ConvNet(num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, num_hidden_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)) \n\u001b[1;32m     82\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvNet' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_units, output_dim):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_features, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_hidden_units, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_hidden_units, out_channels=output_dim, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).permute(0, 2, 1)  # Change shape to [batch_size, channels, length]\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.squeeze(2)  # Remove the last dimension after pooling\n",
    "        return x\n",
    "\n",
    "\n",
    "def info_nce_loss(features, labels, temperature=1.0, threshold=0.3):\n",
    "    # Normalize features to unit length for cosine similarity\n",
    "    features = F.normalize(features, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity matrix\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    \n",
    "    # Calculate Euclidean distances between labels for defining positive pairs\n",
    "    labels_diff = torch.cdist(labels[:,0].unsqueeze(1), labels[:,0].unsqueeze(1))\n",
    "    \n",
    "    # Create a mask for positive samples based on a threshold in label space\n",
    "    positive_mask = (labels_diff <= threshold).float()\n",
    "\n",
    "    # Calculate exponentiated similarities scaled by temperature\n",
    "    exp_sim = torch.exp(similarity_matrix / temperature)\n",
    "    \n",
    "    # Compute sums of exponentiated similarities where masks apply\n",
    "    pos_sum = torch.sum(exp_sim * positive_mask, dim=1)\n",
    "    all_sum = torch.sum(exp_sim, dim=1)\n",
    "    \n",
    "    # Calculate the InfoNCE loss\n",
    "    loss = -torch.log(pos_sum / all_sum + 1e-6)\n",
    "\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# Example usage in a training loop\n",
    "# Assume `features` and `labels` are outputs from your model and target labels respectively\n",
    "# loss = info_nce_loss(features, labels, temperature=1.0, threshold=0.1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, optimizer\n",
    "model = ConvNet(num_features=120, num_hidden_units=16, output_dim=3).to(device)\n",
    "model.encoder.load_state_dict(torch.load('pretrained_model.pth', map_location=device)) \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd5a30b-d181-4313-8088-abe95efcb013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConvNet\u001b[49m(num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, num_hidden_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# load pre-trained encoder\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConvNet' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc14821-3adf-4f55-87b8-4c515a1a07e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 10, Loss: 0.9970\n",
      "Pretrain Epoch 20, Loss: 0.9909\n",
      "Pretrain Epoch 30, Loss: 0.9885\n",
      "Pretrain Epoch 40, Loss: 0.9848\n",
      "Pretrain Epoch 50, Loss: 0.9832\n",
      "Contrastive Epoch 10, Loss: 1.9103\n",
      "Contrastive Epoch 20, Loss: 1.9009\n",
      "Contrastive Epoch 30, Loss: 1.8992\n",
      "Contrastive Epoch 40, Loss: 1.8960\n",
      "Contrastive Epoch 50, Loss: 1.8989\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the ConvNet with VAE-like architecture\n",
    "class ConvNetVAE(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_units, latent_dim):\n",
    "        super(ConvNetVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_features, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_hidden_units, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=num_hidden_units, out_channels=latent_dim, kernel_size=5, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=latent_dim, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=num_hidden_units, out_channels=num_hidden_units, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=num_hidden_units, out_channels=num_features, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).permute(0, 2, 1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded.squeeze(2), decoded.permute(0, 2, 1).squeeze(1)\n",
    "\n",
    "# Reconstruction loss definition\n",
    "def reconstruction_loss(reconstructed, original):\n",
    "    return F.mse_loss(reconstructed, original)\n",
    "\n",
    "# DataLoader setup\n",
    "neural_data = torch.randn(10178, 120)  # Simulated neural data (timesteps, features)\n",
    "labels = torch.randn(10178, 3)         # Simulated continuous labels\n",
    "dataset = TensorDataset(neural_data, labels)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Pretrain the VAE model\n",
    "def pretrain_epoch(loader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, _ in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, reconstructed = model(data)\n",
    "        loss = reconstruction_loss(reconstructed, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 10\n",
    "model = ConvNetVAE(num_features=120, num_hidden_units=16, latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Run pretraining\n",
    "pretrain_epochs = 50\n",
    "for epoch in range(pretrain_epochs):\n",
    "    loss = pretrain_epoch(loader, model, optimizer)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Pretrain Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save the pre-trained encoder model\n",
    "torch.save(model.encoder.state_dict(), 'encoder_pretrained.pth')\n",
    "\n",
    "# Load the encoder for contrastive learning\n",
    "model.encoder.load_state_dict(torch.load('encoder_pretrained.pth'))\n",
    "\n",
    "# Define the InfoNCE loss function for contrastive learning\n",
    "def info_nce_loss(features, labels, temperature=1.0, threshold=0.3):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    labels_diff = torch.cdist(labels[:,0].unsqueeze(1), labels[:,0].unsqueeze(1))\n",
    "    positive_mask = (labels_diff <= threshold).float()\n",
    "    exp_sim = torch.exp(similarity_matrix / temperature)\n",
    "    pos_sum = torch.sum(exp_sim * positive_mask, dim=1)\n",
    "    all_sum = torch.sum(exp_sim, dim=1)\n",
    "    loss = -torch.log(pos_sum / all_sum + 1e-6)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# Training loop for contrastive learning\n",
    "def train_contrastive_epoch(loader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, targets in loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        features, _ = model(data)  # Use only encoder outputs\n",
    "        loss = info_nce_loss(features, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Run contrastive training\n",
    "contrastive_epochs = 50\n",
    "for epoch in range(contrastive_epochs):\n",
    "    loss = train_contrastive_epoch(loader, model, optimizer)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Contrastive Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f45bdd-7ac5-4185-91fb-5a1c3d14c9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
